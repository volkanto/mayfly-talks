= What about Performance Testing?

- ... in the UserStory environment?
- ... on Production?
- ... on cross-production?
- ... on cross-production per featureset?

[NOTE.speaker]
--
A combination of multiple solutions should give enough insight.
--

== UserStory Environment
Script based performance-tests for each story.

* Pros
** Isolated environment & changeset (master+1)
** Small scope with stubs
** Huge incentive -> DoD
* Cons
** Testing takes time
** Behaviour of virtual environments could be unpredictable

[NOTE.speaker]
--
Unfortunately testing takes time. You should limit the scope of what you're testing.
If you have a performance test of several hours, you'll block your own process.
It is unknown (to us) how well systems resources can be sharded in virtual environments.
This solution either stands of falls with that.
--

== Production
Measurements based on alerting and monitoring

* Pros
** Real usage, predictable resources
** No test scripts to maintain
* Cons
** Fluctuating peak load
** Everything must be measured
** Continuous deployments interfere
** System-as-a-whole test

[NOTE.speaker]
--
The biggest problems are that:
- you cannot take the holiday load into account.
- when a cascading problem arises, you're most probably too late to fix it.
--

== Cross-Production

Single high load shadow environment

* Pros
** Peak load can be simulated
** Stable resources
* Cons
** Continuous deployments interfere
** Problem-focus causes test-debt

[NOTE.speaker]
--
Testing on (cross)-production can be a valid method since the Mayfly process makes disrupting the Sprint less intrusive.
Since your stable branch is always master, you can park other work and start fixing the (performance) issue at hand.
The context switch should also be acceptable since in your mind you were working on Master+1.
You cannot focus on one problem without creating test-debt for the features to come
--

== Cross-Production per featureset

Multi high load shadow environments per featureset

* Pros
** Peak load can be simulated
** Fixed set of features per run
** Allows automatic pinpointing
* Cons:
** Behaviour of virtual environments could be unpredictable
** Requires all dependencies to run in cluster

[NOTE.speaker]
--
Method: Shadow environment per set of features (time based)
with constant load using testscripts
Based on the versions of your application,
 you can define the changelog between versions.
If you test too many features at once,
 you simply make smaller slices.
If you need to pinpoint a problem, you could do an automatic bisect
and run the specific test until you find the feature responsible
Just like with the UserStory environment,
we do not (yet) know how virtual resources behave under load
--

= So...?

- ... in the UserStory environment!
- ... on Production!
- ... on cross-production!
- ... on cross-production per featureset!