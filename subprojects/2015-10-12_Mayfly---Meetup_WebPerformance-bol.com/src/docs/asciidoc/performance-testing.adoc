= What about Performance Testing?

[NOTE.speaker]
--
A combination of multiple solutions should give enough insight.
--

- ... in the UserStory environment?
- ... on Production?
- ... on cross-production?
- ... on cross-production per featureset?

== In the UserStory Environment

Method: Delta based performance-tests for each story with testscripts.

[NOTE.speaker]
--
Unfortunately testing takes time. You should limit the scope of what you're testing.
If you have a performance test of several hours, you'll block your own process.
It is unknown (to us) how well systems resources can be sharded in virtual environments.
This solution either stands of falls with that.
--

Pros:
- Isolated (predictable?) environment
- Small scope, minimal is one component with stubs
- Small changeset; UserStory is always master+1
- Huge incentive -> Definition of Done

Cons:
- Performance Testing takes time
- Behaviour of virtual environments could be unpredictable

== On Production

Method: Measurements based on alerting and monitoring

[NOTE.speaker]
--
The biggest problems are that:
- you cannot take the holiday load into account.
- when a cascading problem arises, you're most probably too late to fix it.
--

Pros:
- Real usage
- No test scripts to maintain
- Amount of resources is stable

Cons:
- Peak load differs throughout the year
- Measure everything, complete with A/B testing
- Hard to pinpoint the problem due to continuous deployments
- Testing the system as a whole instead of components

== On Cross-Production

Method: One shadow environment with constant load and deployments using testscripts

[NOTE.speaker]
--
Testing on (cross)-production can be a valid method since the Mayfly process makes disrupting the Sprint less intrusive.
Since your stable branch is always master, you can park other work and start fixing the (performance) issue at hand.
The context switch should also be acceptable since in your mind you were working on Master+1.
--

Pros:
- Peak load can be simulated
- Amount of resources is stable
- Compatible with old infrastructure

Cons:
- Hard to pinpoint the problem due to continuous deployments
- New features continuously stream to production.
- You cannot focus on one problem without creating test-debt for the features to come

== On Cross-Production per featureset

[NOTE.speaker]
--
Based on the versions of your application you can define the changelog between versions.
If you test too many features at once, you simply make smaller slices.
If you need to pinpoint a problem, you could do an automatic bisect and run the specific test until you find the feature responsible
Just like with the UserStory environment, we do not (yet) know how virtual resources behave under load
--

Method: Shadow environment per set of features (time based) with constant load using testscripts

Pros:
- Peak load can be simulated
- Environment should be predictable
- Fixed set of features per run, easier to locate problems

Cons:
- Behaviour of virtual environments could be unpredictable
- Requires all dependencies to run in cluster
